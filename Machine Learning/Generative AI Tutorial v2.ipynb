{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI\n",
    "\n",
    "Tutorial from: https://www.youtube.com/watch?v=L8U-pm-vZ4c\n",
    "\n",
    "Link to Project: https://docs.pinecone.io/docs/abstractive-question-answering\n",
    "\n",
    "Need to install this:  \n",
    "    \n",
    "pip install -qU datasets pinecone-client sentence-transformers torch pip install -qU datasets pinecone-client sentence-transformers torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "Our source data will be taken from the Wiki Snippets dataset, which contains over 17 million passages from Wikipedia. But, since indexing the entire dataset may take some time, we will only utilize 50,000 passages in this demo that include \"History\" in the \"section title\" column. If you want, you may utilize the complete dataset. Pinecone vector database can effortlessly manage millions of documents for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\urso_luis_a@lilly.com\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset from huggingface in streaming mode and shuffle it\n",
    "wiki_data = load_dataset(\n",
    "    'vblagoje/wikipedia_snippets_streamed',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ").shuffle(seed=960)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we iteratively download records one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wiki_id': 'Q7649565',\n",
       " 'start_paragraph': 20,\n",
       " 'start_character': 272,\n",
       " 'end_paragraph': 24,\n",
       " 'end_character': 380,\n",
       " 'article_title': 'Sustainable Agriculture Research and Education',\n",
       " 'section_title': \"2000s & Evaluation of the program's effectiveness\",\n",
       " 'passage_text': \"preserving the surrounding prairies. It ran until March 31, 2001.\\nIn 2008, SARE celebrated its 20th anniversary. To that date, the program had funded 3,700 projects and was operating with an annual budget of approximately $19 million. Evaluation of the program's effectiveness As of 2008, 64% of farmers who had received SARE grants stated that they had been able to earn increased profits as a result of the funding they received and utilization of sustainable agriculture methods. Additionally, 79% of grantees said that they had experienced a significant improvement in soil quality though the environmentally friendly, sustainable methods that they were\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the contents of a single document in the dataset\n",
    "next(iter(wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only documents with History as section_title\n",
    "history = wiki_data.filter(\n",
    "    lambda d: d['section_title'].startswith('History')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through the dataset and apply our filter to select the 50,000 historical passages. We will extract article_title, section_title and passage_text from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [09:17<00:00, 89.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # progress bar\n",
    "\n",
    "total_doc_count = 50000\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "# iterate through the dataset and apply our filter\n",
    "for d in tqdm(history, total=total_doc_count):\n",
    "    # extract the fields we need\n",
    "    doc = {\n",
    "        \"article_title\": d[\"article_title\"],\n",
    "        \"section_title\": d[\"section_title\"],\n",
    "        \"passage_text\": d[\"passage_text\"]\n",
    "    }\n",
    "    # add the dict containing fields we need to docs list\n",
    "    docs.append(doc)\n",
    "\n",
    "    # stop iteration once we reach 50k\n",
    "    if counter == total_doc_count:\n",
    "        break\n",
    "\n",
    "    # increase the counter on every iteration\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>section_title</th>\n",
       "      <th>passage_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taupo District</td>\n",
       "      <td>History</td>\n",
       "      <td>was not until the 1950s that the region starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sutarfeni</td>\n",
       "      <td>History &amp; Western asian analogues</td>\n",
       "      <td>Sutarfeni History strand-like pheni were Phena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bishop Wand Church of England School</td>\n",
       "      <td>History</td>\n",
       "      <td>The Bishop Wand Church of England School Histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Teufelsmoor</td>\n",
       "      <td>History &amp; Situation today</td>\n",
       "      <td>made to preserve the original landscape, altho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surface Hill Uniting Church</td>\n",
       "      <td>History</td>\n",
       "      <td>in perpetual reminder that work and worship go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Electras (band)</td>\n",
       "      <td>History</td>\n",
       "      <td>as its B-side. However, copies of the single, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Swanton House</td>\n",
       "      <td>History</td>\n",
       "      <td>it. Lane provided funds for restoration by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Takashinohama Line</td>\n",
       "      <td>History</td>\n",
       "      <td>Takashinohama Line The Takashinohama Line (高師浜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tamil Methodist Church</td>\n",
       "      <td>History</td>\n",
       "      <td>Tamil Methodist Church History The church was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Star Music</td>\n",
       "      <td>History</td>\n",
       "      <td>in order to strengthen its production base and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              article_title   \n",
       "0                            Taupo District  \\\n",
       "1                                 Sutarfeni   \n",
       "2  The Bishop Wand Church of England School   \n",
       "3                               Teufelsmoor   \n",
       "4               Surface Hill Uniting Church   \n",
       "5                       The Electras (band)   \n",
       "6                             Swanton House   \n",
       "7                        Takashinohama Line   \n",
       "8                    Tamil Methodist Church   \n",
       "9                                Star Music   \n",
       "\n",
       "                       section_title   \n",
       "0                            History  \\\n",
       "1  History & Western asian analogues   \n",
       "2                            History   \n",
       "3          History & Situation today   \n",
       "4                            History   \n",
       "5                            History   \n",
       "6                            History   \n",
       "7                            History   \n",
       "8                            History   \n",
       "9                            History   \n",
       "\n",
       "                                        passage_text  \n",
       "0  was not until the 1950s that the region starte...  \n",
       "1  Sutarfeni History strand-like pheni were Phena...  \n",
       "2  The Bishop Wand Church of England School Histo...  \n",
       "3  made to preserve the original landscape, altho...  \n",
       "4  in perpetual reminder that work and worship go...  \n",
       "5  as its B-side. However, copies of the single, ...  \n",
       "6  it. Lane provided funds for restoration by the...  \n",
       "7  Takashinohama Line The Takashinohama Line (高師浜...  \n",
       "8  Tamil Methodist Church History The church was ...  \n",
       "9  in order to strengthen its production base and...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a pandas dataframe with the documents we extracted\n",
    "df = pd.DataFrame(docs)\n",
    "df.iloc[0:10,0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pinecone Index\n",
    "\n",
    "The Pinecone index stores vector representations of our historical passages which we can retrieve later using another vector (query vector). To build our vector index, we must first establish a connection with Pinecone. For this, we need an API from Pinecone. You can get one for free from here. You also need to know the environment for your index; for new accounts, the default environment is us-east1-gcp.\n",
    "\n",
    "We initialize the connection as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# connect to pinecone environment\n",
    "pinecone.init(\n",
    "    api_key=\"676be22d-86c1-48cd-af60-688993fe5fc3\",\n",
    "    environment=\"asia-southeast1-gcp-free\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new index. We will name it \"abstractive-question-answering\" — you can name it anything we want. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 768-dimension vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"abstractive-question-answering\"\n",
    "\n",
    "# check if the abstractive-question-answering index exists\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # create the index if it does not exist\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "# connect to abstractive-question-answering index we created\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initialize Retriever\n",
    "\n",
    "Next, we need to initialize our retriever. The retriever will mainly do two things:\n",
    "\n",
    "- Generate embeddings for all historical passages (context vectors/embeddings)\n",
    "- Generate embeddings for our questions (query vector/embedding)\n",
    "\n",
    "The retriever will create embeddings such that the questions and passages that hold the answers to our queries are close to one another in the vector space. We will use a SentenceTransformer model based on Microsoft's MPNet as our retriever. This model performs quite well for comparing the similarity between queries and documents. We can use Cosine Similarity to compute the similarity between query and context vectors generated by this model (Pinecone automatically does this for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# load the retriever model from huggingface model hub\n",
    "retriever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\", device=device)\n",
    "retriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Upsert\n",
    "\n",
    "Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, section title, passage text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [4:27:38<00:00, 20.54s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.3,\n",
       " 'namespaces': {'': {'vector_count': 50001}},\n",
       " 'total_vector_count': 50001}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use batches of 64\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(df))\n",
    "    # extract batch\n",
    "    batch = df.iloc[i:i_end]\n",
    "    # generate embeddings for batch\n",
    "    emb = retriever.encode(batch[\"passage_text\"].tolist()).tolist()\n",
    "    # get metadata\n",
    "    meta = batch.to_dict(orient=\"records\")\n",
    "    # create unique IDs\n",
    "    ids = [f\"{idx}\" for idx in range(i, i_end)]\n",
    "    # add all to upsert list\n",
    "    to_upsert = list(zip(ids, emb, meta))\n",
    "    # upsert/insert these records to pinecone\n",
    "    _ = index.upsert(vectors=to_upsert)\n",
    "\n",
    "# check that we have all vectors in index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Generator\n",
    "\n",
    "We will use ELI5 BART for the generator which is a Sequence-To-Sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-To-Sequence models can take a text sequence as input and produce a different text sequence as output.\n",
    "\n",
    "The input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token <P>, so the input string will look as follows:\n",
    "\n",
    "question: What is a sonic boom? context: <P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. <P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. <P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.\n",
    "\n",
    "More detail on how the ELI5 dataset was built is available here and how ELI5 BART model was trained is available here.\n",
    "\n",
    "Let's initialize the BART model using transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.64MB/s]\n",
      "c:\\Users\\urso_luis_a@lilly.com\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\urso_luis_a@lilly.com\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.22MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 27.0/27.0 [00:00<00:00, 27.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.32k/1.32k [00:00<00:00, 1.32MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [02:23<00:00, 11.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# load bart tokenizer and model from huggingface\n",
    "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
    "generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the components of our abstract QA system are complete and ready to be queried. But first, let's write some helper functions to retrieve context passages from Pinecone index and to format the query in the way the generator expects the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query, top_k):\n",
    "    # generate embeddings for the query\n",
    "    xq = retriever.encode([query]).tolist()\n",
    "    # search pinecone index for context passage with the answer\n",
    "    xc = index.query(xq, top_k=top_k, include_metadata=True)\n",
    "    return xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query(query, context):\n",
    "    # extract passage_text from Pinecone search result and add the <P> tag\n",
    "    context = [f\"<P> {m['metadata']['passage_text']}\" for m in context]\n",
    "    # concatinate all context passages\n",
    "    context = \" \".join(context)\n",
    "    # contcatinate the query and context passages\n",
    "    query = f\"question: {query} context: {context}\"\n",
    "    return query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the helper functions. We will query the Pinecone index function we created earlier with the query_pinecone to get context passages and pass them to the format_query function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '3593',\n",
       "              'metadata': {'article_title': 'Electric power system',\n",
       "                           'passage_text': 'Electric power system History In '\n",
       "                                           '1881, two electricians built the '\n",
       "                                           \"world's first power system at \"\n",
       "                                           'Godalming in England. It was '\n",
       "                                           'powered by two waterwheels and '\n",
       "                                           'produced an alternating current '\n",
       "                                           'that in turn supplied seven '\n",
       "                                           'Siemens arc lamps at 250 volts and '\n",
       "                                           '34 incandescent lamps at 40 volts. '\n",
       "                                           'However, supply to the lamps was '\n",
       "                                           'intermittent and in 1882 Thomas '\n",
       "                                           'Edison and his company, The Edison '\n",
       "                                           'Electric Light Company, developed '\n",
       "                                           'the first steam-powered electric '\n",
       "                                           'power station on Pearl Street in '\n",
       "                                           'New York City. The Pearl Street '\n",
       "                                           'Station initially powered around '\n",
       "                                           '3,000 lamps for 59 customers. The '\n",
       "                                           'power station generated direct '\n",
       "                                           'current and',\n",
       "                           'section_title': 'History'},\n",
       "              'score': 0.69118011,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"when was the first electric power system built?\"\n",
    "result = query_pinecone(query, top_k=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('question: when was the first electric power system built? context: <P> '\n",
      " \"Electric power system History In 1881, two electricians built the world's \"\n",
      " 'first power system at Godalming in England. It was powered by two '\n",
      " 'waterwheels and produced an alternating current that in turn supplied seven '\n",
      " 'Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. '\n",
      " 'However, supply to the lamps was intermittent and in 1882 Thomas Edison and '\n",
      " 'his company, The Edison Electric Light Company, developed the first '\n",
      " 'steam-powered electric power station on Pearl Street in New York City. The '\n",
      " 'Pearl Street Station initially powered around 3,000 lamps for 59 customers. '\n",
      " 'The power station generated direct current and')\n"
     ]
    }
   ],
   "source": [
    "# format the query in the form generator expects the input\n",
    "query = format_query(query, result[\"matches\"])\n",
    "pprint(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Answers\n",
    "\n",
    "The output looks great. Now let's write a function to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query):\n",
    "    # tokenize the query to get input_ids\n",
    "    inputs = tokenizer([query], max_length=1024, return_tensors=\"pt\")\n",
    "    # use generator to predict output ids\n",
    "    ids = generator.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=40)\n",
    "    # use tokenizer to decode the output ids\n",
    "    answer = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return pprint(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The first electric power system was built in 1881 at Godalming in England. '\n",
      " 'It was powered by two waterwheels and produced alternating current that in '\n",
      " 'turn supplied seven Siemens arc lamps')\n"
     ]
    }
   ],
   "source": [
    "generate_answer(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the generator used the provided context to answer our question. Let's run some more queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The first wireless message was sent in 1866 by Mahlon Loomis, who had a kite '\n",
      " 'on a mountaintop 14 miles apart. The kite was connected to a cable')\n"
     ]
    }
   ],
   "source": [
    "query = \"How was the first wireless message sent?\"\n",
    "context = query_pinecone(query, top_k=5)\n",
    "query = format_query(query, context[\"matches\"])\n",
    "generate_answer(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that this answer is correct, we can check the contexts used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerous Native American Mounds and evidence of strong settlements still being discovered.\n",
      "Named after George Washington, the first President of the United States of America, the area was first settled by those seeking both economic and political freedom in this frontier land of vast timber and mineral resources. Inland waterway transportation brought about heavy river settlements. The arrival of railroads in the late 1800s boosted economic, social and political developments.\n",
      "Vernon, the geographical center of the county derives is named for George Washington's Virginia home, Mt. Vernon. The pioneer town was also the site of a major Indian settlement.\n",
      "The county courthouse was\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for doc in context[\"matches\"]:\n",
    "    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COVID-19 is a zoonotic disease, which means that it is a virus that is '\n",
      " 'transmitted from one animal to another. It is not a virus that can be '\n",
      " 'transmitted from person')\n"
     ]
    }
   ],
   "source": [
    "query = \"where did COVID-19 originate?\"\n",
    "context = query_pinecone(query, top_k=3)\n",
    "query = format_query(query, context[\"matches\"])\n",
    "generate_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to establish with certainty which diseases jumped from other animals to humans, but there is increasing evidence from DNA and RNA sequencing, that measles, smallpox, influenza, HIV, and diphtheria came to humans this way. Various forms of the common cold and tuberculosis also are adaptations of strains originating in other species.\n",
      "Zoonoses are of interest because they are often previously unrecognized diseases or have increased virulence in populations lacking immunity. The West Nile virus appeared in the United States in 1999 in the New York City area, and moved through the country in the summer of 2002, causing much distress. Bubonic\n",
      "---\n",
      "plague is a zoonotic disease, as are salmonellosis, Rocky Mountain spotted fever, and Lyme disease.\n",
      "A major factor contributing to the appearance of new zoonotic pathogens in human populations is increased contact between humans and wildlife. This can be caused either by encroachment of human activity into wilderness areas or by movement of wild animals into areas of human activity. An example of this is the outbreak of Nipah virus in peninsular Malaysia in 1999, when intensive pig farming began on the habitat of infected fruit bats. Unidentified infection of the pigs amplified the force of infection, eventually transmitting the virus\n",
      "---\n",
      "man killed and twenty-nine died of disease.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for doc in context[\"matches\"]:\n",
    "    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s finish with a final few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The War of Currents was a series of events in the early 1900s between Edison '\n",
      " 'and Westinghouse. The two companies were competing for the market share of '\n",
      " 'electric power in the United States')\n"
     ]
    }
   ],
   "source": [
    "query = \"what was the war of currents?\"\n",
    "context = query_pinecone(query, top_k=5)\n",
    "query = format_query(query, context[\"matches\"])\n",
    "generate_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The first person to walk on the moon was Neil Armstrong in 1969. He walked '\n",
      " 'on the moon in 1969. He was the first person to walk on the moon.')\n"
     ]
    }
   ],
   "source": [
    "query = \"who was the first person on the moon?\"\n",
    "context = query_pinecone(query, top_k=10)\n",
    "query = format_query(query, context[\"matches\"])\n",
    "generate_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Space Shuttle was the most expensive project in the history of NASA. It '\n",
      " 'cost about $10 billion to build.')\n"
     ]
    }
   ],
   "source": [
    "query = \"what was NASAs most expensive project?\"\n",
    "context = query_pinecone(query, top_k=3)\n",
    "query = format_query(query, context[\"matches\"])\n",
    "generate_answer(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
