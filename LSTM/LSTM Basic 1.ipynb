{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "## Long-Short Team Memory\n",
    "##\n",
    "### Tutorial based on: https://medium.com/datarisk-io/introdu%C3%A7%C3%A3o-%C3%A0s-redes-lstm-prevendo-valor-de-a%C3%A7%C3%B5es-na-bolsa-df270ca0cee5\n",
    "##\n",
    "## Application\n",
    "<p>\n",
    "The LSTM model can be applied for long temporal series since using other regression models like ARMA or ARIMA creates 1 polynomium for period used in the past to predict the future. Higher granularity increases the prediction model and don't deal with sazionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Recommendation\n",
    "<p>\n",
    " Forecasting: Principles and Practice - Rob J. Hyndman and George Athanasopoulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT STOCK PRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.470001],\n",
       "       [33.549999],\n",
       "       [33.650002],\n",
       "       [33.099998],\n",
       "       [33.869999]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from Stocks and adjust the dataset for training\n",
    "\n",
    "base = pd.read_csv('PETR4_Training.csv')\n",
    "\n",
    "# Remove NA values from the dataset\n",
    "\n",
    "base = base.dropna() \n",
    "\n",
    "# Select only the Opening Data and stora in an Array that will be used for training\n",
    "\n",
    "base_treinamento = base.iloc[:,1:2].values\n",
    "\n",
    "# Inspect the first 5 elementes of the array that will be used for training\n",
    "\n",
    "base_treinamento[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14523493],\n",
       "       [0.1573375 ],\n",
       "       [0.17246654],\n",
       "       [0.08925869],\n",
       "       [0.20574899]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the values of the dataset between 0 and 1\n",
    "\n",
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)\n",
    "\n",
    "\n",
    "# Inspect the first 5 elementes of the scaled training array\n",
    "\n",
    "base_treinamento_normalizada[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_treinamento_normalizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data on the required structure to be trained by the LSTM model. We will define the temporal window to be observed\n",
    "\n",
    "obs_window = 90\n",
    "\n",
    "previsores=[]\n",
    "preco_real=[]\n",
    "\n",
    "# Vary i from obs_window value since this is the window size to observe\n",
    "\n",
    "for i in range(0, 90):\n",
    "    previsores.append(base_treinamento_normalizada[i-90:i,0])\n",
    "    preco_real.append(base_treinamento_normalizada[i,0])\n",
    "    \n",
    "# Transform in the format to be used for training (this is required to make the model work)\n",
    "\n",
    "previsores, preco_real = np.array(previsores), np.array(preco_real)\n",
    "previsores = np.reshape(previsores, (previsores.shape[0], previsores.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionamos também camadas de Dropout para prevenir ovefitting\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (previsores.shape[1], 1)))\n",
    "regressor.add(Dropout(0.3))\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "regressor.add(Dense(units = 1, activation = 'linear'))\n",
    "# Aqui definimos o otimizador, a função de perda e métricas de      \n",
    "# avaliação do modelo durante o treinamento.\n",
    "regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error',\n",
    "                  metrics = ['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTM.call().\n\n\u001b[1mslice index 0 of dimension 1 out of bounds. for '{{node sequential_2_1/lstm_8_1/strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2](data, sequential_2_1/lstm_8_1/strided_slice_1/stack, sequential_2_1/lstm_8_1/strided_slice_1/stack_1, sequential_2_1/lstm_8_1/strided_slice_1/stack_2)' with input shapes: [?,0,1], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <0 1 0>, input[3] = <1 1 1>.\u001b[0m\n\nArguments received by LSTM.call():\n  • sequences=tf.Tensor(shape=(None, 0, 1), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Por fim, damos início ao treinamento. A taxa de aprendizado foi a padrão do otimizador,0.001.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevisores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreco_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LSTM.call().\n\n\u001b[1mslice index 0 of dimension 1 out of bounds. for '{{node sequential_2_1/lstm_8_1/strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2](data, sequential_2_1/lstm_8_1/strided_slice_1/stack, sequential_2_1/lstm_8_1/strided_slice_1/stack_1, sequential_2_1/lstm_8_1/strided_slice_1/stack_2)' with input shapes: [?,0,1], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <0 1 0>, input[3] = <1 1 1>.\u001b[0m\n\nArguments received by LSTM.call():\n  • sequences=tf.Tensor(shape=(None, 0, 1), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
     ]
    }
   ],
   "source": [
    "# Por fim, damos início ao treinamento. A taxa de aprendizado foi a padrão do otimizador,0.001.\n",
    "regressor.fit(previsores, preco_real, epochs = 150, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
